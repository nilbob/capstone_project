{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size = 5>Applied Data Science Capstone Project: <br> \n",
    "    Finding the right spot for a Vegan Lifestyle Centre in Berlin </font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Data](#data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Section <a name=\"data\"></a>\n",
    "To begin with, I will collect all the data that is necessary to tackle the problem. For this project, I will need the boroughs and neighborhoods of Berlin with their longitude and latitude values as well as the Foursquare API venue data for existing vegan/vegetarian restaurants and/or yoga studios, hotels, bars & tourist attractions.\n",
    "\n",
    "#### Boroughs and Neighborhoods of Berlin:\n",
    "\n",
    "1. Firstly, I will use the table from the <a href=\"https://de.wikipedia.org/wiki/Liste_der_Bezirke_und_Ortsteile_Berlins\">Wikipedia Article</a> that lists all 12 boroughs and 96 neighborhoods of Berlin (<a href=\"https://github.com/nilbob/capstone_project/blob/master/boroughs_neighborhoods_berlin.csv\">github_csv_file</a>). Since it is the second table in the article and I'm struggling to scrape the data with beautifulsoup correctly, I copied the data from the article and created a csv file out of it instead. If I find the time to have a closer look into it, I might find a way to scrap that particular table. The most important part of the table is the second column (\"Ortsteile\" = neighborhoods). I will convert the neighborhoods into latitude and longitude values using Geocoder.  \n",
    "3. Moreover, I grab the latitude and longitude value for Berlin to map the neighborhoods on the Berlin map.\n",
    "2. Afterwards, I will visualize the neighborhoods using the Folium library to see which neighborhoods/boroughs are of interest for our objectives (close to the city centre or main attractions but also easy to access for locals). This pre-selection has a huge impact on the outcome of my analysis/project. Therefore, it might be possible that when I, once, have gone through the whole project, I might have to come back to this point and re-adjust the neighborhoods. This will be part of the discussion section to go more into detail about the strength and weaknesses of the project. \n",
    "3. Once, I have found the areas of interest, that I consider to be the best fit, I put them into a pandas dataframe to work with them throughout the project.\n",
    "<br>\n",
    "\n",
    "#### Using the Foursquare API to get the venue data:\n",
    "\n",
    "To get an overview of the the most common venue categories in the interested neighborhoods, I will collect the top 100 venues within a 500 meter radius to check if the categories of interest are among them (vegan/vegetarian restaurants, yoga studios, bio/organic supermarket, hotels, main attractions). To receive all the required data, I create a function to loop through the Foursquare URL Requests for each neighborhood. Once, I've extracted the categories out of the JSON, returned from the Foursquare API, I'll beautify the data to fit it into a pandas dataframe. I will then check for each neighborhood the amount of venues that were returned. Since the neighborhoods in Berlin can be rather big in comparison to New York's or Toronto's, I might have to adjust the radius and extend it to 1000 - 1500 meter and check how many venues are there. A histogram might come in handy to visualize the amount of venues that exist per neighborhood and how likely it is to adjust the radius. That aside, I'm not only interested in how many venues in total exist, but also how many unique categories there are, so that I know what I can work with or get to know how to group them to my need.\n",
    "\n",
    "\n",
    "#### Analysing the data from Foursquare API:\n",
    "To deep dive more into the analysis/data and to prepare the data for the algorithm, I will do a \"one hot encoding\" with the data. That means that I will have the venue categories to be set to false/true (0,1) for each neighborhood. This allows us to find out how often a categorie eg. a vegan/vegetarian restaurant exists for each neighborhood on average. Now, I will be able to further analyse the data and discover the top 10 venue categories for each neighborhood and sort them in descending order to check if the categories that I'm interested in, do exist to solve my business objective to find the best place to set up my vegan centre.\n",
    "\n",
    "#### Cluster the neighborhoods using k-means clustering algorithm:\n",
    "When I'm done with the analysis and preparation of the data, I'll then start running k-means to cluster the neighborhoods into 5 clusters. With the most frequent categories at hand, I will be able to categorize the cluster into meaningful segments to look for the best place that covers most of the categories that match our criterias for preventing too much competition and to set up the business. \n",
    " \n",
    "#### Visualize the data:\n",
    "Having run the clustering, too easier access and understand each cluster, I would visualize each cluster using a bar chart to have closer look at them. Once, I'm satisfied with the data and number of k's, I will use the Folium library to visualize the neighborhoods in Berlin and their emerging clusters to give a recommendation based on my findings.\n",
    "\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
